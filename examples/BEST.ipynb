{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Estimation Supersedes the T-Test\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook is an adapted version of one provided by the PyMC developers (https://www.pymc.io/projects/examples/en/latest/case_studies/BEST.html). Thanks to the original authors: Andrew Straw, Thomas Wiecki, Chris Fonnesbeck, and Andrés Suárez. Here, you can see how to work with bayes-toolbox, and I have also added a toy problem at the end to demonstrate the use of a Bayesian analogoue of a paired t-test. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents:  \n",
    "\n",
    "- [The Problem](#The-Problem)\n",
    "- [Example: Drug trial evaluation](#Example:-Drug-trial-evaluation)\n",
    "- [Running the BEST test using bayes-toolbox](#Running-the-BEST-test-using-bayes-toolbox)\n",
    "- [Extra: Example using paired samples](#Extra:-Example-using-paired-samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import bayes-toolbox's glm module\n",
    "import bayes_toolbox.glm as bg\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "az.rcParams[\"stats.hdi_prob\"] = 0.95\n",
    "rng = np.random.default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "This model replicates the example used in **Bayesian estimation supersedes the t-test** {cite:p}`kruschke2013`.\n",
    "\n",
    "Several statistical inference procedures involve the comparison of two groups. We may be interested in whether one group is larger than another, or simply different from the other. We require a statistical model for this because true differences are usually accompanied by measurement or stochastic noise that prevent us from drawing conclusions simply from differences calculated from the observed data. \n",
    "\n",
    "The *de facto* standard for statistically comparing two (or more) samples is to use a statistical test. This involves expressing a null hypothesis, which typically claims that there is no difference between the groups, and using a chosen test statistic to determine whether the distribution of the observed data is plausible under the hypothesis. This rejection occurs when the calculated test statistic is higher than some pre-specified threshold value.\n",
    "\n",
    "Unfortunately, it is not easy to conduct hypothesis tests correctly, and their results are very easy to misinterpret. Setting up a statistical test involves several subjective choices (*e.g.* statistical test to use, null hypothesis to test, significance level) by the user that are rarely justified based on the problem or decision at hand, but rather, are usually based on traditional choices that are entirely arbitrary {cite:p}`johnson1999`. The evidence that it provides to the user is indirect, incomplete, and typically overstates the evidence against the null hypothesis {cite:p}`goodman1999`. \n",
    "\n",
    "A more informative and effective approach for comparing groups is one based on **estimation** rather than **testing**, and is driven by Bayesian probability rather than frequentist. That is, rather than testing whether two groups are different, we instead pursue an estimate of how different they are, which is fundamentally more informative. Moreover, we include an estimate of uncertainty associated with that difference which includes uncertainty due to our lack of knowledge of the model parameters (epistemic uncertainty) and uncertainty due to the inherent stochasticity of the system (aleatory uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Drug trial evaluation\n",
    "\n",
    "To illustrate how this Bayesian estimation approach works in practice, we will use a fictitious example from {cite:t}`kruschke2013` concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those receiving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "iq_drug = np.array([\n",
    "    101, 100, 102, 104, 102, 97, 105, 105, 98, 101, 100, 123, 105, 103, \n",
    "    100, 95, 102, 106, 109, 102, 82, 102, 100, 102, 102, 101, 102, 102,\n",
    "    103, 103, 97, 97, 103, 101, 97, 104, 96, 103, 124, 101, 101, 100,\n",
    "    101, 101, 104, 100, 101\n",
    "])\n",
    "\n",
    "iq_placebo = np.array([\n",
    "    99, 101, 100, 101, 102, 100, 97, 101, 104, 101, 102, 102, 100, 105,\n",
    "    88, 101, 100, 104, 100, 100, 100, 101, 102, 103, 97, 101, 101, 100,\n",
    "    101, 99, 101, 100, 100, 101, 100, 99, 101, 100, 102, 99, 100, 99\n",
    "])\n",
    "# fmt: on\n",
    "\n",
    "df1 = pd.DataFrame({\"iq\": iq_drug, \"group\": \"drug\"})\n",
    "df2 = pd.DataFrame({\"iq\": iq_placebo, \"group\": \"placebo\"})\n",
    "indv = pd.concat([df1, df2]).reset_index()\n",
    "\n",
    "sns.histplot(data=indv, x=\"iq\", hue=\"group\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in a Bayesian approach to inference is to specify the full probability model that corresponds to the problem. For this example, Kruschke chooses a Student-t distribution to describe the distributions of the scores in each group. This choice adds robustness to the analysis, as a T distribution is less sensitive to outlier observations, relative to a normal distribution. The three-parameter Student-t distribution allows for the specification of a mean $\\mu$, a precision (inverse-variance) $\\lambda$ and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "The degrees-of-freedom parameter essentially specifies the \"normality\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails. Thus, the likelihood functions of our model are specified as follows:\n",
    "\n",
    "$$y^{(treat)}_i \\sim T(\\nu, \\mu_1, \\sigma_1)$$\n",
    "\n",
    "$$y^{(placebo)}_i \\sim T(\\nu, \\mu_2, \\sigma_2)$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups. We will, of course, have separate parameters for the means $\\mu_k, k=1,2$ and standard deviations $\\sigma_k$. Since the means are real-valued, we will apply normal priors on them, and arbitrarily set the hyperparameters to the pooled empirical mean of the data and twice the pooled empirical standard deviation, which applies very diffuse information to these quantities (and importantly, does not favor one or the other *a priori*).\n",
    "\n",
    "$$\\mu_k \\sim N(\\bar{x}, 2s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_m = indv.iq.mean()\n",
    "mu_s = indv.iq.std() * 2\n",
    "\n",
    "with pm.Model() as model:\n",
    "    group1_mean = pm.Normal(\"group1_mean\", mu=mu_m, sigma=mu_s)\n",
    "    group2_mean = pm.Normal(\"group2_mean\", mu=mu_m, sigma=mu_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group standard deviations will be given a uniform prior over a plausible range of values for the variability of the outcome variable, IQ.\n",
    "\n",
    "In Kruschke's original model, he uses a very wide uniform prior for the group standard deviations, from the pooled empirical standard deviation divided by 1000 to the pooled standard deviation multiplied by 1000. This is a poor choice of prior, because very basic prior knowledge about measures of human coginition dictate that the variation cannot ever be as high as this upper bound. IQ is a standardized measure, and hence this constrains how variable a given population's IQ values can be. When you place such a wide uniform prior on these values, you are essentially giving a lot of prior weight on inadmissable values. In this example, there is little practical difference, but in general it is best to apply as much prior information that you have available to the parameterization of prior distributions. \n",
    "\n",
    "We will instead set the group standard deviations to have a $\\text{Uniform}(1,10)$ prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_low = 10**-1\n",
    "sigma_high = 10\n",
    "\n",
    "with model:\n",
    "    group1_std = pm.Uniform(\"group1_std\", lower=sigma_low, upper=sigma_high)\n",
    "    group2_std = pm.Uniform(\"group2_std\", lower=sigma_low, upper=sigma_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow Kruschke by making the prior for $\\nu$ exponentially distributed with a mean of 30; this allocates high prior probability over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    nu_minus_one = pm.Exponential(\"nu_minus_one\", 1 / 29.0)\n",
    "    nu = pm.Deterministic(\"nu\", nu_minus_one + 1)\n",
    "    nu_log10 = pm.Deterministic(\"nu_log10\", np.log10(nu))\n",
    "\n",
    "az.plot_kde(rng.exponential(scale=29, size=10000) + 1, fill_kwargs={\"alpha\": 0.5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PyMC parametrizes the Student-T in terms of precision, rather than standard deviation, we must transform the standard deviations before specifying our likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    lambda_1 = group1_std**-2\n",
    "    lambda_2 = group2_std**-2\n",
    "    group1 = pm.StudentT(\"drug\", nu=nu, mu=group1_mean, lam=lambda_1, observed=iq_drug)\n",
    "    group2 = pm.StudentT(\"placebo\", nu=nu, mu=group2_mean, lam=lambda_2, observed=iq_placebo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fully specified our probabilistic model, we can turn our attention to calculating the comparisons of interest in order to evaluate the effect of the drug. To this end, we can specify deterministic nodes in our model for the difference between the group means and the difference between the group standard deviations. Wrapping them in named `Deterministic` objects signals to PyMC that we wish to record the sampled values as part of the output. As a joint measure of the groups, we will also estimate the \"effect size\", which is the difference in means scaled by the pooled estimates of standard deviation. This quantity can be harder to interpret, since it is no longer in the same units as our data, but the quantity is a function of all four estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    diff_of_means = pm.Deterministic(\"difference of means\", group1_mean - group2_mean)\n",
    "    diff_of_stds = pm.Deterministic(\"difference of stds\", group1_std - group2_std)\n",
    "    effect_size = pm.Deterministic(\n",
    "        \"effect size\", diff_of_means / np.sqrt((group1_std**2 + group2_std**2) / 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit the model and evaluate its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Running the BEST test using `bayes-toolbox`\n",
    "\n",
    "Now, instead of having to code up your statistical model from scratch, you only have to call the appropriate function (in this case, BEST, as a tribute to the original authors). \n",
    "\n",
    "Note that bayes-toolbox tries to enforce the use of [tidy data](https://vita.had.co.nz/papers/tidy-data.pdf) by requiring the two inputs to be the outcome variable and the grouping variable, rather than entering separate variables for the two groups' outcomes (e.g., y1 and y2). Also, you can increase the number of draws by setting the \"n_draws\" argument to whatever value you wish. It defaults to 1000 draws per chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bg, idata_bg = bg.BEST(indv[\"iq\"], indv[\"group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the stochastic parameters of the model. Arviz's `plot_posterior` function replicates the informative histograms portrayed in {cite:p}`kruschke2013`. These summarize the posterior distributions of the parameters, and present a 95% credible interval and the posterior mean. The plots below are constructed with the final 1000 samples from each of the 2 chains, pooled together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    idata_bg,\n",
    "    var_names=[\"group_mean\", \"group_std\", \"nu\", \"nu_log10\"],\n",
    "    kind=\"hist\",\n",
    "    round_to=3,\n",
    "    bins=50\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the group differences below, we can conclude that there are meaningful differences between the two groups for all three measures. For these comparisons, it is useful to use zero as a reference value (`ref_val`); providing this reference value yields cumulative probabilities for the posterior distribution on either side of the value. Thus, for the difference of means, at least 97% of the posterior probability are greater than zero, which suggests the group means are credibly different. The effect size and differences in standard deviation are similarly positive.\n",
    "\n",
    "These estimates suggest that the \"smart drug\" increased both the expected scores, but also the variability in scores across the sample. So, this does not rule out the possibility that some recipients may be adversely affected by the drug at the same time others benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    idata_bg,\n",
    "    var_names=[\"difference of means\", \"difference of stds\", \"effect size\"],\n",
    "    ref_val=0,\n",
    "    color=\"#87ceeb\",\n",
    "    kind=\"hist\",\n",
    "    round_to=3,\n",
    "    bins=50\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `plot_forest` is called on a trace with more than one chain, it also plots the potential scale reduction parameter, which is used to reveal evidence for lack of convergence; values near one, as we have here, suggest that the model has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(idata_bg, var_names=[\"group_mean\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd rather see the combined estimates from all chains, set the \"combined\" argument to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(idata_bg, var_names=[\"group_std\", \"nu\"], combined=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata_bg, var_names=[\"difference of means\", \"difference of stds\", \"effect size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Example using paired samples\n",
    "\n",
    "Now, let's imagine a scenario where me make measurements prior to and following some intervention. In other words, each \"participant\" serves as their own control. This is perhaps one of the most common experimental designs. I've copied some fake data from [here](https://www.geeksforgeeks.org/how-to-conduct-a-paired-samples-t-test-in-python/) and first report the results of a standard frequentist paired t-test and then show the much richer set of information you can easily extract using bayes-toolbox's paired samples version of the BEST test. \n",
    "\n",
    "In this example, we're looking at the change in gas mileage pre- and post- application of a new engine oil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Pre holds the mileage before applying the different engine oil\n",
    "pre = np.array([30, 31, 34, 40, 36, 35, 34, 30, 28, 29])\n",
    "\n",
    "# Post holds the mileage after applying the different engine oil\n",
    "post = np.array([30, 31, 32, 38, 32, 31, 32, 29, 28, 30])\n",
    "\n",
    "# Performing the paired sample t-test\n",
    "stats.ttest_rel(pre, post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using scipy's `ttest_rel` we get the test statistic and p-value. Now, calling on bayes-toolbox's version of a paired t-test and combining the InferenceData object that is returned with Arviz's `plot_posterior`, we can visualize the full marginalized posteriors, which include not only point estimates but also the uncertainty around those estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paired_a, idata_paired_a = bg.BEST_paired(pre, post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(\n",
    "    idata_paired_a,\n",
    "    var_names=[\"mu\", \"effect_size\"],\n",
    "    ref_val=0,\n",
    "    kind=\"hist\",\n",
    "    round_to=3,\n",
    "    bins=50\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can enter difference scores and compare directly to zero\n",
    "y = pre - post\n",
    "\n",
    "model_paired_b, idata_paired_b = bg.BEST_paired(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should see same result as previous model\n",
    "az.plot_posterior(\n",
    "    idata_paired_b,\n",
    "    var_names=[\"mu\", \"effect_size\"],\n",
    "    ref_val=0,\n",
    "    kind=\"hist\",\n",
    "    round_to=3,\n",
    "    bins=50\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_paired_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paired_b.named_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(idata, az.data.inference_data.InferenceData)\n",
    "assert \"mu\" in model_paired_b.named_vars\n",
    "assert \"sigma\" in model_paired_b.named_vars\n",
    "assert \"nu_minus_one\" in model_paired_b.named_vars\n",
    "assert \"nu\" in model_paired_b.named_vars\n",
    "assert \"likelihood\" in model_paired_b.named_vars\n",
    "assert \"effect_size\" in model_paired_b.named_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Authored by Andrew Straw in Dec, 2012 ([best](https://github.com/strawlab/best))\n",
    "* Ported to PyMC3 by Thomas Wiecki in 2015\n",
    "* Updated by Chris Fonnesbeck in Dec, 2020\n",
    "* Ported to PyMC4 by Andrés Suárez in Ene, 2022 ([pymc-examples#52](https://github.com/pymc-devs/pymc-examples/issues/52))\n",
    "* Edited and added to by Hyosub Kim in 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "bayes_toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
